{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intermediate-discretion",
   "metadata": {},
   "source": [
    "# Memo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "african-feature",
   "metadata": {},
   "source": [
    "memo about SISG4HEIAlpha\n",
    "\n",
    "# make layout -> make activity -> make travel pattern -> PIR or door sensor\n",
    "# in GUI_Layout\n",
    "Fur_list = [['Bed', 'Wardrobe', 'Desk and Chair'], ['Kitchen Stove', 'Cupboard', 'Refrigerator',\n",
    "            'Wash Machine', 'Trash Bin'], ['Sofa and TV', 'Table and Chair']]\n",
    "# in GUI_Semantics\n",
    "Dict_ZF = {'Resting': ['Bed', 'Wardrobe', 'Nightstand', 'Desk and Chair', 'Random'],\n",
    "           'Living': ['Table and Chair', 'Sofa and TV', 'Random'],\n",
    "           'Cooking': ['Kitchen Stove', 'Cupboard', 'Refrigerator', 'Trash Bin', 'Wash Machine', 'Random']}\n",
    "S_r, N, stat, stat_Incre = FP_main.generate_house(N, topo, varss)\n",
    "\n",
    "House, Toil_Bath, Furnitures, Doors, Walls, T_con, B_con, type = FLP.Floor_Plan(topo, furs)\n",
    "\n",
    "FSave.Height_SampleSave(type, self.rooms, self.T_Bs, self.furnitures, root_path+'/DataBase/'+s_path)\n",
    "\n",
    "# in FSave\n",
    "Fur_choice = [['WAR', '000'], ['De', '00'], ['So', '00'], ['DTA', '000'], ['KS', '00'],\n",
    "                  ['CB', '00'], ['RFA', '000'], ['WM', '00'], ['TB', '00']]\n",
    "                  \n",
    "# in Dest_nodes.py\n",
    "Furniture2Activity = {'Bedroom':{'Bed':'Sleep', 'Wardrobe': 'Dress_up', 'Chair':'Work'},\n",
    "                      'Livingroom':{'Sofa':'Watch_TV', 'Chair':'Dining'},\n",
    "                      'Kitchen': {'Kitchen_Stove':'Cooking0', 'Cupboard':'Cooking1',\n",
    "                                    'Refrigerator':'Cooking2', 'Wash_Machine':'Washing',\n",
    "                                    'Trash_Bin':'Cleaning'}}\n",
    "Door2Activity = {'Entrance':'Go_out', 'Toilet_Door':'Go_to_Toilet', 'Bathroom_Door':'Go_to_Bathroom'}\n",
    "\n",
    "self.Actseqs = HAS_main.actseq_gen(self.savelist, self.paras[6], self.paras[0], self.intvar, self.paras[1:4], self.paras[4:6], self.order)\n",
    "\n",
    "HASave.Mark2Txt(Actseqs[i], Savelist[i], stimes[i], etimes[i])\n",
    "\n",
    "# Actseqs: list of activity sequnces, Actseqs[i] = activity sequence of i-th layout, \n",
    "# Actseqs[i][j] = [j-th activity, duration time of j-th activity] in i-th layout,\n",
    "# activity = [ 0 Sleep_Noon, 1 Sleep_Evening, 2 Wash_Self(in Bathroom), 3 Cooking0(in KS), 4 Cooking1(in CB), \n",
    "#              5 Cooking2(in RFA), 6 Eat, 7 Bath, 8 Dress_up, 9 Go_out, \n",
    "#              10 Toilet_Short, 11Toilet_Long, 12 Clean, 13 Work, 14 Watch_TV, \n",
    "#              15 Wash_Clothing(in WM), 16 Wandering, 17 Relax], \n",
    "# (activity sequences = [0 sleep_noon, 1 sleep, 2 eat, 3 bath, 4 go_out, \n",
    "#                        5 toilet(short), 6 toilet(long), 7 clean, 8 work, 9 watch_TV, \n",
    "#                        10 Washing, 11 Wandering, 12 Resting])\n",
    "#  Save those as Num2Act = ['Sleep', 'Sleep', 'Wash and Brush', 'Cook', 'Take TableWare', \n",
    "#                           'Take Food', 'Eat', 'Bath', 'Dress up', 'Go out', \n",
    "#                           'Go to Toilet', 'Go to Toilet', 'Clean', 'Read', 'Watch TV', \n",
    "#                           'Wash Clothes', 'Wander', 'Relax']\n",
    "    \n",
    "Load.load_actseq(h_v, fname0, fname1)\n",
    "# Load.processact(act, fname0) (like {'Sleep': 'Sleep', 'Wash and Brush': 'Go_to_Bathroom', 'Cook': 'Cooking0', 'Bath': 'Go_to_Bathroom',\n",
    "#     'Go out': 'Go_out', 'Go to Toilet': 'Go_to_Toilet', 'Wash Clothes': 'Washing', 'Watch TV': 'Watch_TV'})\n",
    "#     converts ACTS.txt into activity code for travel pattern generation\n",
    "    \n",
    "# Tools_.PIR_position decide the limit of the number of the sensors\n",
    "\n",
    "\n",
    "# \n",
    "│  │  ├─1456,1040,2080,180,738LL\n",
    "│  │  │      ACTS.txt\n",
    "│  │  │      Cleaning_distance.csv\n",
    "│  │  │      Cooking0_distance.csv\n",
    "│  │  │      Cooking1_distance.csv\n",
    "│  │  │      Cooking2_distance.csv\n",
    "│  │  │      Destinations.json\n",
    "│  │  │      Dining_distance.csv\n",
    "│  │  │      Discomfortable_value.csv\n",
    "│  │  │      Dress_up_distance.csv\n",
    "│  │  │      Go_out_distance.csv\n",
    "│  │  │      Go_to_Bathroom_distance.csv\n",
    "│  │  │      Go_to_Toilet_distance.csv\n",
    "│  │  │      Height_Function.json\n",
    "│  │  │      Human_Path,55,right.json\n",
    "│  │  │      Max_Distances.json\n",
    "│  │  │      Semantic.json\n",
    "│  │  │      Sleep_distance.csv\n",
    "│  │  │      Watch_TV_distance.csv\n",
    "│  │  │      Work_distance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-setting",
   "metadata": {},
   "source": [
    "# Memo about significant change"
   ]
  },
  {
   "cell_type": "raw",
   "id": "second-myrtle",
   "metadata": {},
   "source": [
    "time : change\n",
    "2021-10-22-08-34 : hidden location_sequence, so input of utils.path_generate and utils.multi_sensor_records don't need location_sequence. TP[i] add 3rd element 'end time of the path'(= start time of the action that is the goal of i-th path)\n",
    "2021-11-12 : add Sensor class, add home_equipment as intance value of Activity class\n",
    "2022-05-25 : use floor_plan module\n",
    "\n",
    "        Resident's walking trajectories between activities in the layout.\n",
    "        TP[i][0](string)    = i-th path information, \"kind_of_walking,start_place_code,end_place_code,random_seed\"\n",
    "        TP[i][1](list)      = [body coordinates, left step coordinates, right step coordinates] about movement of i-th path\n",
    "        TP[i][2](timedelta) = the end time of the path (= the start time of the activity that is done at the goal of i-th path)\n",
    "        e.g. TP = [('Direct,Bed,Desk,60', [body_c, left_f, right_f], end_time),\n",
    "                   ('Wander,Sofa,Dinner_Table,Bed,Wardrobe,33', [body_c, left_f, right_f], end time),\n",
    "                   ...]\n",
    "        body_c is like [(0.00,0.00), (1.03, 1.21), (2.44, 3.34), ..., (100.23, 43.78)]. left_f and right_f is similar form of body_c.\n",
    "-> WalkingTrajectory\n",
    "\n",
    "main changing points in 2022/07/28\n",
    "activity_sequence : list of tuple -> list of ActivityDataPoint\n",
    "activity_scheduling -> generate_activity_sequence\n",
    "path_generate -> generate_walking_trajectories\n",
    "sensor_info -> sensors : list of Sensor\n",
    "\n",
    "2022/12/01\n",
    "Toil -> WC\n",
    "So -> Sofa\n",
    "Cb -> CB\n",
    "Cha -> C\n",
    "DTa -> DT\n",
    "WaR -> WR\n",
    "NSt -> NS\n",
    "Rfa -> RF\n",
    "TB -> T\n",
    "\n",
    "2023/08/22\n",
    "add a 'src' folder\n",
    "rename 'new_functions.py' 'utils.py'\n",
    "rename 'anomaly.py' 'anomaly_model.py'\n",
    "use sklearn-crfsuite instead of ace-sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-creek",
   "metadata": {},
   "source": [
    "## Possible improvements\n",
    "### Memo\n",
    "- [x] （travel pattern の行動中の動きについて、うまく全部繋げられるか？現状次の行動の開始地点と前の行動の終了地点が一致しない） -> しようと思えば、終了地点と開始地点を揃えられるが、行動内で生じた誤差として考えてもいい気がする。\n",
    "- [x] utils.update_multi_sensor_log 内の past_x, past_y により赤外線センサが人を動いているかを判定するところで,現状sampling_duration秒前の地点との差分を取っているが，もしもsampling_duration = 10 とすると，10秒前の地点との差分になってしまう->PIRセンサのサンプリングレートが長すぎるとそもそも実用的でないから、妥当な仮定のような気がする。\n",
    "‐ [x] 経路計算のための distance の情報を計算した後に保存する際のsave json.dumps に MyEncoder 形式で保存するように変更した。また、generate_layout_with_necessary_furniture が繰り返しgenerate_layout を実行する際に、前のデータが完全に上書きされているかどうか確認していない（たぶん全部上書きされているため、不都合は起きないはず。）\n",
    "- [x] 転倒には30秒かかるので、walkingが重ならないように、location[i]からlocation[i+1]へのpathで転倒する際には、location[i+1]での行動の行動時間が十分長い物を選ぶようにしている．\n",
    "- [x] 時系列データを (start time of the parameter, value of the parameter) のリストで表現するか、(start time of the parameter, end time of the parameter, value of the parameter)のリストで表現するか。前者は情報量が削減でき，矛盾のチェックが時系列順に並んでいるかどうかだけで済む。後者は最後の期間のパラメータの終了時刻を明示できる（前者ではインデックスがiのパラメータはiの開始時刻からi+1の開始時刻までになるが、リストの最後の要素のパラメータは次の要素がないため、終了時刻がわからない.最後にダミー要素として0の値を持つ期間にすればなんとかなる？）。-> 後者にする．\n",
    "- [x] フォルダ構造から再考（SISG4HEIALpha の階層的フォルダ構造いる？名前で(layout1)とか毎回指定してもよくない？）フォルダ名にある情報は全部 Semantic.json に入れる．\n",
    "- [x] テキスト形式の保存に加えて pickle 等を用いて異常ラベルなどを保存する\n",
    "- [x] ログのテキストファイルを入力として，リストのオブジェクトに読み込む方法\n",
    "- [x] フォルダ名に家具や大きさいる？layoutだけ残すか，もしくは全てフラットにするか？その代わり，家具一覧や大きさ一覧は Semantic.json に組み込む.\n",
    "- [x] utils.direct_path をきれいにする\n",
    "- [x] PIRと圧力と家電センサのログからそれらを一つにまとめたものを作る関数\n",
    "- [x] 移動経路のGUI足跡シミュレータ\n",
    "- [x] 行動や異常の各パラメータ一覧を出力、保存 (pack)、再利用 (unpack) できる関数 -> pickle を使った\n",
    "\n",
    "### Priorities\n",
    "- [ ] footprint_simulator で、 ドアセンサが上手く反映できていない。（現時点で、歩行中の時間のみ描画しており、歩行前後で起こるドアセンサの反応が描画範囲外になっている）\n",
    "- [ ] utils 内の ActivityDataPoint などは activity_model に、WalkingTrajectory などは（新ファイルの） walking_model に分離したほうがよいか？activity_model　と anomaly_model で循環インポートになりそう。\n",
    "- [ ] update_states_of_motion_sensors で temp_body_radius の変数名を間違えていたため、転倒時の体の幅が 2023/08/02 以前に作成したデータでは正確に反映されていない。 \n",
    "- [ ] anomaly_model.forgetting_labels が遅い．頑張れば速くできそう\n",
    "- [ ] 睡眠行動にトイレが断続的に入るパターンなどをモデル化する際には、現状の last_activity や sleep_after_24では対応できない。そのため、次の日の行動列も保存し、次の日の生成時にそれをコピーできるように作り変える必要がある。それか，睡眠にトイレ行動としてのサブ行動を許すか？\n",
    "- [ ] 新しい家具の追加など、レイアウトの修正を後から簡単にできるようにする。\n",
    "- [ ] Activity の所の 'Bed' などの場所名は floorplan とかに定数としておかなくて大丈夫か\n",
    "- [ ] MMSE の幅が 1 month 以外でも使いやすいようにする\n",
    "\n",
    "\n",
    "### Floor plan\n",
    "- [ ] 自作クラス Zone, Wall, Furniture の活用\n",
    "- [ ] dis_val のindex と実際のx, y の関係をわかりやすく。（参考、disval を作成する関数、または、Tools_.cal_I_J）\n",
    "- [ ] ワンルームでなくて、壁がある場合はどうするのか？単純にdisval を100に設定するのか？2階など複雑な住居はどうするのか？\n",
    "- [ ] 間取りを修正できるようにする．\n",
    "\n",
    "### Activity schedule\n",
    "- [ ] 行動列の円グラフ「1日の0:00から3日の0:00まで」とかの切りのいい時はうまくいくが、それ以外の微妙な時刻からの開始・終了にも対応する\n",
    "- [ ] activity sequence 生成の行動の開始時刻や継続時間のサンプリングの記述を整頓する。例として、Activity.sampling_duration にはremainder と開始時間を入力に取るようにしたり、基本行動の開始時刻のサンプリングを切断正規分布を用いたり、day_schedule.update_schedule 内に短い行動を防ぐ例外処理をまとめたりする。\n",
    "- [ ] 拡張1. 既に行動 B と C が決まっている場合、行動 A を (1) もしくは (2) のどちらかの条件を満たすように入れる。 (1) A が B と C の間にないといけない。 (2) A が B と C の間にあってはいけない。 拡張2. (1)行動 A が行動 B から T_c [分]よりも離れている。(2) 行動 A が行動 B からT_c [分]以内に作られる。拡張1と拡張2を合わせて考えると、拡張 1_2 : 行動 A が行動 B の前（または後ろ）の M[分]以内に存在する（または存在しない）。を実装できればよい？ただし、拡張1_2の形式にすると、夕食後睡眠前に取る行動Aを実装するには、事前にMを指定できず、既にサンプリングした夕食の終了時刻と睡眠の開始時刻に依存する。\n",
    "- [ ] 拡張した行動をもう少しわかりやすい形で整頓する（MetaActivity をわかりやすくする。）\n",
    "- [ ] activity.dertermine_place() の Notes に書かれているように、行動場所の選択にランダム性を追加する拡張。\n",
    "- [ ] 基本行動間の分布の重なり度合いから，リサンプリングがうまくいきそうかどうかを事前に警告する．FundamentalActivity.sampling_start_time() 内のlower_limitの例外に該当しそうかどうかもこの事前のうちに済ませておく．\n",
    "\n",
    "### Walking trajectory\n",
    "- [ ] 異なるwalking 同士が重なってしまうエラー（A->B->C の移動において，A->B と B->C が重なってしまう現象）の改善方法。案1. 極端に短い行動時間の行動をうまく消す？ 案2. 移動経路を作成する段階で、もしも移動が重なる場合、後ろの行動の開始時刻をずらすことで対応する。修正した行動列も一緒に出力する。\n",
    "\n",
    "### Sensors\n",
    "- [ ] ドアセンサ実装\n",
    "- [ ] 圧力センサが行動終了時に消すか消さないかをセンサデータ生成時に行動の種類によって分ける。例として、睡眠（ベッドに上る）やトイレ，外出（そこにとどまらない）は圧力センサを切る，反対に調理や机の下などはセンサをオンにするなど.\n",
    "- [ ] 現状家電センサは行動時間中にセンサがついているとしているが，次の行動への歩き初めには消したり，行動時間中でもランダムにつけたり消したりするようにする\n",
    "- [ ] センサごとにサンプリングレートが異なる拡張\n",
    "- [ ] 各センサの出力をベルヌーイ分布などで誤差を加える．\n",
    "\n",
    "### Anomalies\n",
    "- [ ] 転倒に関して、転びやすい場所を discomfortable value を用いて実現できるか？\n",
    "- [ ] sensor_model.Sensor とActivity.home_equipment と calculate_sensor_record_of_a_home_equipment あたりの家電センサの記述を連携させられるか\n",
    "- [ ] 現状閉じこもりと準寝たきりが同時に発生した時の外出回数の減少は、準寝たきりによる減少を優先している。同時に（行動の統計量に影響を与える）複数の異常が発生した時の優先順位をどう決めるかをわかりやすくしたい。\n",
    "\n",
    "### Others\n",
    "- [ ] start, end を持つ時間区間に関してのクラス utils.TimeInterval を活用するか？duration()やincludes(t)（timedeltaオブジェクトがstart, endの間に入るか）などの関数がある。\n",
    "- [ ] 全体的にパラメータ名をもう少し短く簡潔にし、docstring をきれいに書く。\n",
    "- [ ] 足跡シミュレータが指定した時間から始められるようにする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-belize",
   "metadata": {},
   "source": [
    "# Import libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jewish-police",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# self-made\n",
    "import src.analysis as analysis\n",
    "import src.anomaly_model as anomaly_model\n",
    "import src.utils as utils\n",
    "\n",
    "working_path = Path().resolve()\n",
    "layout_data_path = working_path / \"layout_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec884b-2a3a-4168-a269-4a5342488d27",
   "metadata": {},
   "source": [
    "# Analyze long-term data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7c3da-8206-41b4-913d-cb812175190d",
   "metadata": {},
   "source": [
    "## data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abf1471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.timedelta(seconds=20529, microseconds=400000), 23, True), (datetime.timedelta(seconds=20529, microseconds=900000), 11, True), (datetime.timedelta(seconds=20530, microseconds=700000), 23, False)]\n",
      "3.89 MB\n",
      "dict_keys(['MMSE', 'being housebound', 'being semi-bedridden', 'forgetting', 'wandering', 'fall while walking', 'fall while standing'])\n",
      "(62208000, 29)\n",
      "(62208000, 6)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "['being semi-bedridden', 'being housebound', 'forgetting', 'wandering', 'fall while walking', 'fall while standing']\n"
     ]
    }
   ],
   "source": [
    "# Load sensor data and anomaly labels\n",
    "\n",
    "data_folder_name = \"test_data_3\"\n",
    "path = Path(layout_data_path / \"test_layout\" / data_folder_name)  # data path\n",
    "data_save_path = path / \"experiment\"\n",
    "\n",
    "if not data_save_path.exists():\n",
    "    data_save_path.mkdir()\n",
    "\n",
    "SD = utils.pickle_load(path, \"SD\")\n",
    "AL = utils.pickle_load(path, \"AL\")\n",
    "AL_periods = analysis.make_AL_periods(path)\n",
    "print(SD[:3])\n",
    "print(analysis.memory_size(SD))\n",
    "print(AL.keys())\n",
    "\n",
    "# make data of matrix form\n",
    "\n",
    "start = timedelta(days=0)\n",
    "end = timedelta(days=360 * 2)\n",
    "seconds = 1\n",
    "duration = timedelta(seconds=seconds)\n",
    "_type = \"raw\"\n",
    "(SD_mat, SD_names, AL_mat, AL_names) = analysis.matrix_with_discretized_time_interval(\n",
    "    SD, AL_periods, start, end, duration, _type=_type\n",
    ")\n",
    "print(SD_mat.shape)\n",
    "print(AL_mat.shape)\n",
    "print(SD_names)\n",
    "print(AL_names)\n",
    "\n",
    "utils.pickle_dump(data_save_path, \"SD_mat\" + \"_\" + _type + \"_\" + str(seconds), SD_mat)\n",
    "utils.pickle_dump(data_save_path, \"AL_mat\" + \"_\" + _type + \"_\" + str(seconds), AL_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6560421-2e38-4a24-89ba-d9571da3a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20593092, 28) matrices. 279933588 / 279935939.\n",
      "(20593092, 6)\n",
      "[14721, 14722, 14723, 14724, 14725]\n"
     ]
    }
   ],
   "source": [
    "# reduce invalid data which takes same value in fixed-length windows.\n",
    "\n",
    "_type = \"raw\"\n",
    "data_folder_name = \"test_data_2\"\n",
    "path = Path(layout_data_path / \"test_layout\" / data_folder_name)\n",
    "SD_mat_name = f\"SD_mat_{_type}_1\"\n",
    "SD_names_name = f\"SD_names\"\n",
    "AL_mat_name = f\"AL_mat_{_type}_1\"\n",
    "AL_names_name = \"AL_names\"\n",
    "SD_mat = utils.pickle_load(path / \"experiment1\", SD_mat_name)\n",
    "SD_names = utils.pickle_load(path / \"experiment1\", SD_names_name)\n",
    "AL_mat = utils.pickle_load(path / \"experiment1\", AL_mat_name)\n",
    "AL_names = utils.pickle_load(path / \"experiment1\", AL_names_name)\n",
    "\n",
    "valid_indexes = set()\n",
    "\n",
    "\n",
    "def is_invalid4raw(data):\n",
    "    return np.all(data == 0)\n",
    "\n",
    "\n",
    "def is_invalid4LF(data):\n",
    "    return np.all(data == data[0])\n",
    "\n",
    "\n",
    "w = 61\n",
    "half_w = int((w - 1) / 2)\n",
    "start_index = half_w\n",
    "end_index = int(SD_mat.shape[0] - half_w)\n",
    "total_len = int(SD_mat.shape[0] - w)\n",
    "diff = int(total_len / 100000)\n",
    "for i in range(start_index, end_index):\n",
    "    if i % diff == 0:\n",
    "        utils.print_progress_bar(total_len, i, \"Making reduced matrices.\")\n",
    "    data = SD_mat[i - half_w : i + half_w + 1]\n",
    "    if _type == \"raw\":\n",
    "        if not (is_invalid4raw(data)):\n",
    "            valid_indexes |= set(range(i - half_w, i + half_w + 1))\n",
    "    if _type == \"last-fired\":\n",
    "        if not (is_invalid4LF(data)):\n",
    "            valid_indexes |= set(range(i - half_w, i + half_w + 1))\n",
    "\n",
    "valid_indexes = sorted(list(valid_indexes))\n",
    "reduced_SD_mat = SD_mat[valid_indexes]\n",
    "reduced_AL_mat = AL_mat[valid_indexes]\n",
    "save_name_SD = \"reduced_\" + SD_mat_name\n",
    "save_name_AL = f\"reduced_AL_mat_{_type}_1\"\n",
    "utils.pickle_dump(path / \"experiment1\", save_name_SD, reduced_SD_mat)\n",
    "utils.pickle_dump(path / \"experiment1\", save_name_AL, reduced_AL_mat)\n",
    "print(reduced_SD_mat.shape)\n",
    "print(reduced_AL_mat.shape)\n",
    "print(valid_indexes[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81014679-ee4d-4867-9537-1881ea70af26",
   "metadata": {},
   "source": [
    "## Sequential labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70634c09-89a3-450e-b022-ea427c00c33a",
   "metadata": {},
   "source": [
    "### Housebound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4602306-64a4-4ff4-84d4-4a6673e117a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.timedelta(days=266), datetime.timedelta(days=280, seconds=70253, microseconds=514589))\n",
      "(datetime.timedelta(days=702), datetime.timedelta(days=719, seconds=55739, microseconds=224338))\n",
      "(datetime.timedelta(days=1018), datetime.timedelta(days=1035, seconds=15049, microseconds=379638))\n",
      "(datetime.timedelta(days=1002), datetime.timedelta(days=1016, seconds=81198, microseconds=195390))\n",
      "(datetime.timedelta(days=1138), datetime.timedelta(days=1152, seconds=47716, microseconds=528149))\n",
      "(datetime.timedelta(days=1173), datetime.timedelta(days=1186, seconds=7744, microseconds=455161))\n",
      "(datetime.timedelta(days=2114), datetime.timedelta(days=2127, seconds=28233, microseconds=639207))\n",
      "(datetime.timedelta(days=2338), datetime.timedelta(days=2350, seconds=64477, microseconds=942797))\n",
      "(datetime.timedelta(days=2602), datetime.timedelta(days=2616, seconds=71600, microseconds=110643))\n",
      "(datetime.timedelta(days=3048), datetime.timedelta(days=3068, seconds=6532, microseconds=197505))\n",
      "(datetime.timedelta(days=3237), datetime.timedelta(days=3246, seconds=82228, microseconds=728670))\n"
     ]
    }
   ],
   "source": [
    "# histogram of go-out frequency\n",
    "\n",
    "data_folder_name = \"test_data_3\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "train_AS = utils.pickle_load(path, \"AS\")\n",
    "step = timedelta(days=1)\n",
    "start = timedelta(days=100)\n",
    "end = timedelta(days=300)\n",
    "go_out_counts = analysis.generate_block_time_histogram_of_activities(\n",
    "    train_AS, [\"Go out\"], step, start=start, end=end, target=\"frequency\"\n",
    ")\n",
    "x_list = [d.days for d in utils.date_generator(start, end, step)]\n",
    "plt.bar(x_list, go_out_counts)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "AL_periods = analysis.make_AL_periods(path)\n",
    "for p in AL_periods[anomaly_model.BEING_HOUSEBOUND]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649b7a4a-d2bc-4d02-bdb3-36cce467c024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False]\n"
     ]
    }
   ],
   "source": [
    "_type = \"raw\"\n",
    "train_SD = utils.pickle_load(\n",
    "    layout_data_path / \"test_layout\" / \"test_data_3\" / \"experiment\", f\"SD_mat_{_type}_1\"\n",
    ")\n",
    "print(train_SD[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58fbc61d-d804-4b85-a731-ce7415d1cf73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate go out frequency 6.0 / 7.0.\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_go_out_freq_by_PIR(data, sensor_id):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        Duration time between data[i-1] and data[i] is 1 second.\n",
    "    sensor_id : int\n",
    "    step : datetime.timedelta\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts : list of int\n",
    "        Counts[i] is the number of go out in ith day.\n",
    "    \"\"\"\n",
    "    seconds_of_day = 60 * 60 * 24\n",
    "    max_days = data.shape[0] / seconds_of_day\n",
    "    counts = []\n",
    "    past_time = None\n",
    "    num = 0\n",
    "    for i, x in enumerate(data):\n",
    "        if x[sensor_id]:\n",
    "            present_time = timedelta(seconds=i)\n",
    "            if past_time != None:\n",
    "                if present_time - past_time > timedelta(minutes=3):\n",
    "                    num += 1\n",
    "            past_time = present_time\n",
    "        temp = x.copy()\n",
    "        temp[2] = False\n",
    "        if np.any(temp):\n",
    "            past_time = None\n",
    "\n",
    "        if (i != 0) and (i % sec_of_day == 0):\n",
    "            utils.print_progress_bar(\n",
    "                max_days, i / sec_of_day, \"estimate go out frequency\"\n",
    "            )\n",
    "            counts.append(num)\n",
    "            num = 0\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "def estimate_go_out_freq_by_door(data, sensor_id, step = timedelta(seconds = 1)):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "    sensor_id : int\n",
    "    step : datetime.timedelta\n",
    "        Length of time interval in data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts : list of int\n",
    "        Counts[i] is the number of go out in ith day.\n",
    "    \"\"\"\n",
    "    max_days = timedelta(seconds = data.shape[0] * (step / timedelta(seconds = 1))).days\n",
    "    counts = []\n",
    "    past_time = None\n",
    "    num = 0\n",
    "    for i, x in enumerate(data):\n",
    "        if x[sensor_id]:\n",
    "            present_time = timedelta(seconds=i)\n",
    "            if past_time != None:\n",
    "                if present_time - past_time > timedelta(minutes=3):\n",
    "                    num += 1\n",
    "            past_time = present_time\n",
    "        temp = x.copy()\n",
    "        temp[2] = False\n",
    "        if np.any(temp):\n",
    "            past_time = None\n",
    "\n",
    "        if (i != 0) and (i % sec_of_day == 0):\n",
    "            utils.print_progress_bar(\n",
    "                max_days, i / sec_of_day, \"estimate go out frequency\"\n",
    "            )\n",
    "            counts.append(num)\n",
    "            num = 0\n",
    "\n",
    "    return counts\n",
    "\n",
    "estimate_go_out_freq(train_SD[: 60 * 60 * 24 * 7], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7370c88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 86400 * 3\n",
    "step = timedelta(seconds = 1)\n",
    "timedelta(seconds = a *  (step / timedelta(seconds = 1))).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6104e88c-9550-43f5-b724-2585ee911d25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_out_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ae671-b681-4e15-ae68-0098c949f4f8",
   "metadata": {},
   "source": [
    "### Dynamic naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb67fd2c-2f2e-496a-aa0e-a0844fba9ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74363690   694387]c naive Bayes. 75200000 / 75254508.\n",
      " [   81934   114497]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.99      0.99  75058077\n",
      "        True       0.14      0.58      0.23    196431\n",
      "\n",
      "    accuracy                           0.99  75254508\n",
      "   macro avg       0.57      0.79      0.61  75254508\n",
      "weighted avg       1.00      0.99      0.99  75254508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# learn dynamic naive Bayes\n",
    "_type = \"raw\"\n",
    "data_folder_name = \"test_data_1\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "training_SD = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "training_AL = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "# SD_names = utils.pickle_load(path / 'experiment1', 'SD_names')\n",
    "# AL_names = utils.pickle_load(path / 'experiment1', 'AL_names')\n",
    "\n",
    "NB = analysis.NaiveBayes()\n",
    "NB.use_log_prob = True\n",
    "NB.fit(training_SD, training_AL[:, 3])\n",
    "\n",
    "# print(NB.predict_states(training_SD[:30]))\n",
    "\n",
    "# test Naive Bayes\n",
    "data_folder_name = \"test_data_2\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "test_SD = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "test_AL = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "# test_SD_names = utils.pickle_load(path / 'experiment1', 'SD_names')\n",
    "# test_AL_names = utils.pickle_load(path / 'experiment1', 'AL_names')\n",
    "\n",
    "(y_pred, prob) = NB.predict_states(test_SD)\n",
    "y_true = test_AL[:, 3]\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f452f6-6f60-4b77-b2c0-41c45e2a646e",
   "metadata": {},
   "source": [
    "### Hidden Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5153de96-cb58-49fa-bf94-0c679aa3f5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[73852137  1205940] in HMM. 75200000 / 75254508.\n",
      " [     642   195789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99  75058077\n",
      "        True       0.14      1.00      0.25    196431\n",
      "\n",
      "    accuracy                           0.98  75254508\n",
      "   macro avg       0.57      0.99      0.62  75254508\n",
      "weighted avg       1.00      0.98      0.99  75254508\n",
      "\n",
      "1505.18 [s].\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# learn HMM\n",
    "_type = \"raw\"\n",
    "data_folder_name = \"test_data_1\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "train_SD = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "train_AL = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "train_SD_names = utils.pickle_load(path / \"experiment1\", \"SD_names\")\n",
    "train_AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "\n",
    "hmm = analysis.HMM4binary_sensors()\n",
    "hmm.use_log_prob = True\n",
    "hmm.fit_with_true_hidden_states(train_SD, train_AL[:, 3])\n",
    "\n",
    "# test HMM\n",
    "data_folder_name = \"test_data_2\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "test_SD = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "test_AL = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "test_SD_names = utils.pickle_load(path / \"experiment1\", \"SD_names\")\n",
    "test_AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "\n",
    "temp_time = time.time()\n",
    "\n",
    "(y_pred, prob) = hmm.predict_states(test_SD)\n",
    "\n",
    "# print(hmm.C)\n",
    "# print(hmm.A)\n",
    "# print(hmm.P)\n",
    "\n",
    "y_true = test_AL[:, 3]\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"{:.2f} [s].\".format(time.time() - temp_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abd8fe6-4da5-401b-a526-b4e4e6fab361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n",
      " 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28 28\n",
      " 28 28 28 28 28 28 28 28 28 28 28 28 23 22  2  3  2  3  4  4  5  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "# learn HMM\n",
    "_type = \"last-fired\"\n",
    "data_folder_name = \"test_data_1\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "reduced_SD_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "reduced_AL_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "SD_names = utils.pickle_load(path / \"experiment1\", \"SD_names\")\n",
    "AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "LF_vec = analysis.LF_mat2vec(reduced_SD_mat)\n",
    "print(LF_vec[:100])\n",
    "\n",
    "hmm = analysis.HMM4categorical()\n",
    "hmm.fit_with_true_hidden_states(LF_vec, reduced_AL_mat[:, 3])\n",
    "\n",
    "# test HMM\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "_type = \"last-fired\"\n",
    "data_folder_name = \"test_data_2\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "test_SD = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "test_AL = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "test_SD_names = utils.pickle_load(path / \"experiment1\", \"SD_names\")\n",
    "test_AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "test_LF_vec = analysis.LF_mat2vec(test_SD)\n",
    "temp_time = time.time()\n",
    "\n",
    "(state, prob) = hmm.predict_states(test_LF_vec)\n",
    "\n",
    "print(hmm.C)\n",
    "print(hmm.A)\n",
    "print(hmm.P)\n",
    "\n",
    "y_true = test_AL[:, 3]\n",
    "y_pred = state\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"{:.2f} [s].\".format(time.time() - temp_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e89da9d-785b-4c70-b497-1fac62a1ad83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training of HMM likelihood classifier\n",
    "from hmmlearn import hmm\n",
    "\n",
    "_type = \"last-fired\"\n",
    "data_folder_name = \"test_data_1\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "SD_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "AL_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "SD_names = utils.pickle_load(path / f\"experiment1\", \"SD_names\")\n",
    "AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "LF_vec = analysis.LF_mat2vec(SD_mat)\n",
    "HMM = analysis.HMM_likelihood_classifier()\n",
    "n_components = 18\n",
    "HMM.fit(n_components, SD_mat, AL_mat[:, 3])\n",
    "\n",
    "# Test of HMM likelihood classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "_type = \"last-fired\"\n",
    "data_folder_name = \"test_data_2\"\n",
    "path = layout_data_path / \"test_layout\" / data_folder_name\n",
    "test_SD_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_SD_mat_{_type}_1\")\n",
    "test_AL_mat = utils.pickle_load(path / \"experiment1\", f\"reduced_AL_mat_{_type}_1\")\n",
    "SD_names = utils.pickle_load(path / \"experiment1\", \"SD_names\")\n",
    "test_AL_names = utils.pickle_load(path / \"experiment1\", \"AL_names\")\n",
    "w = 61\n",
    "half_w = int((w - 1) / 2)\n",
    "y_pred = HMM.sequential_labeling(test_SD_mat, w)\n",
    "y_true = test_AL_mat[half_w : int(test_SD_mat.shape[0] - half_w), 3]\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a298c17c-7969-44be-98a6-d33d1eab0492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single one-dimensional time-series data\n",
      "0.18904007841994036\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Single multi-dimensional time-series data\n",
      "\n",
      "Multiple one-dimensional time-series data\n",
      "0.5544686707439423\n",
      "[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n",
      "\n",
      "Multiple multi-dimensional time-series data\n",
      "6.937229228472007e-06\n",
      "[1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanakai\\miniconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hidden Markov model\n",
    "# tutorial of hmmlearn: https://hmmlearn.readthedocs.io/en/latest/tutorial.html\n",
    "# hmmlearn for multi-output sequences: https://github.com/hmmlearn/hmmlearn/issues/128\n",
    "# multi-dimensional output: https://stackoverflow.com/questions/17487356/hidden-markov-model-for-multiple-observed-variables\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Single one-dimensional time-series data\n",
    "print(\"Single one-dimensional time-series data\")\n",
    "X_train = np.random.randint(0, 1, size=(100, 1))\n",
    "X_test = np.random.randint(0, 1, size=(10, 1))\n",
    "model = hmm.CategoricalHMM(n_components=2).fit(X_train)\n",
    "logprob, state_sequence = model.decode(X_test)\n",
    "print(np.exp(logprob))\n",
    "print(state_sequence)\n",
    "\n",
    "# Single multi-dimensional time-series data  ->  Dynamic naive bayes classifier\n",
    "print(\"\\nSingle multi-dimensional time-series data\")\n",
    "X_train = np.random.randint(0, 1, size=(100, 2))\n",
    "X_test = np.random.randint(0, 1, size=(10, 2))\n",
    "\n",
    "# Multiple one-dimensional time-series data\n",
    "print(\"\\nMultiple one-dimensional time-series data\")\n",
    "X_train_1 = np.random.randint(0, 1, size=(100, 1)).reshape((100, 1))\n",
    "X_train_2 = np.random.randint(0, 1, size=(200, 1)).reshape((200, 1))\n",
    "X_train = np.concatenate([X_train_1, X_train_2])\n",
    "lengths = np.array([len(X_train_1), len(X_train_2)])\n",
    "X_test = np.random.randint(0, 1, size=(20, 1))\n",
    "model = hmm.CategoricalHMM(n_components=2).fit(X_train, lengths)\n",
    "logprob, state_sequence = model.decode(X_test)\n",
    "print(np.exp(logprob))\n",
    "print(state_sequence)\n",
    "\n",
    "# Multiple multi-dimensional time-series data\n",
    "print(\"\\nMultiple multi-dimensional time-series data\")\n",
    "X_train_1 = np.random.random((10, 3))\n",
    "X_train_2 = np.random.random((5, 3))\n",
    "X_train = np.concatenate([X_train_1, X_train_2])\n",
    "lengths = np.array([len(X_train_1), len(X_train_2)])\n",
    "model = hmm.GaussianHMM(n_components=2).fit(\n",
    "    X_train, lengths\n",
    ")  # multidimensional gaussian\n",
    "\n",
    "X_test = np.array([[0, 1, 0], [1, 0, 0]])\n",
    "logprob, state_sequence = model.decode(X_test)\n",
    "print(np.exp(logprob))\n",
    "print(state_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf19ffb7-ce4c-4619-a6df-2c870537fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 100)\n",
      "125\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ...  True  True  True]]\n",
      "[False  True False  True False False]\n"
     ]
    }
   ],
   "source": [
    "# windows matrix\n",
    "\n",
    "path = Path(layout_data_path / \"test_layout\" / \"test_data_1\")  # data path\n",
    "\n",
    "indexes = list(range(28))\n",
    "\n",
    "time = timedelta(seconds=41904)\n",
    "duration = timedelta(seconds=10)\n",
    "rate = timedelta(seconds=0.1)\n",
    "mat = analysis.window_matrix(SD, time, duration, rate, indexes)\n",
    "\n",
    "print(mat.shape)\n",
    "print(np.sum(mat))\n",
    "print(mat)\n",
    "\n",
    "a = analysis.label_vector(AL_periods, timedelta(days=77, seconds=29020))\n",
    "print(a)\n",
    "\n",
    "analysis.matrix2image(path, \"window_matrix\", mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b65def-5f59-44f5-9bd4-0ef57d52b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:12\n"
     ]
    }
   ],
   "source": [
    "# matrix before a wandering\n",
    "wandering_index = 50\n",
    "test_wandering = AL_periods[anomaly_model.WANDERING][wandering_index]\n",
    "print(test_wandering[1] - test_wandering[0])\n",
    "indexes = list(range(28))\n",
    "duration = timedelta(seconds=60)\n",
    "rate = timedelta(seconds=0.1)\n",
    "sum_mat = analysis.window_matrix(SD, test_wandering[0], duration, rate, indexes)\n",
    "sum_mat = np.zeros((sum_mat.shape[0], sum_mat.shape[1]))\n",
    "count = 0\n",
    "for t in utils.date_generator(\n",
    "    test_wandering[0], test_wandering[1], timedelta(seconds=1)\n",
    "):\n",
    "    sum_mat += analysis.window_matrix(SD, t, duration, rate, indexes)\n",
    "    count += 1\n",
    "analysis.matrix2image(\n",
    "    path, \"before_wandering{}\".format(wandering_index), sum_mat / count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aba34e-289b-4560-a867-c7cf0e338248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix before a wandering\n",
    "wandering_index = 0\n",
    "test_wandering = AL_periods[anomaly_model.WANDERING][wandering_index]\n",
    "print(test_wandering[1] - test_wandering[0])\n",
    "indexes = list(range(28))\n",
    "duration = timedelta(seconds=60)\n",
    "rate = timedelta(seconds=0.1)\n",
    "sum_mat = analysis.window_matrix(SD, test_wandering[0], duration, rate, indexes)\n",
    "sum_mat = np.zeros((sum_mat.shape[0], sum_mat.shape[1]))\n",
    "count = 0\n",
    "for t in utils.date_generator(\n",
    "    test_wandering[0], test_wandering[1], timedelta(seconds=1)\n",
    "):\n",
    "    sum_mat += analysis.window_matrix(SD, t, duration, rate, indexes)\n",
    "    count += 1\n",
    "analysis.matrix2image(\n",
    "    path, \"before_wandering{}\".format(wandering_index), sum_mat / count\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f5ec71-5cad-4513-8999-ead239913f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the co-occurrence matrix of labels. 3239 days, 2:59:00 / 3240 days, 0:00:00..\r"
     ]
    }
   ],
   "source": [
    "# label correlation\n",
    "start_time = timedelta(days=0)\n",
    "end_time = timedelta(days=9 * 360)\n",
    "count = 0\n",
    "sum_mat = np.zeros((6, 6))\n",
    "progress_bar_step = timedelta(days=1)\n",
    "temp_time = start_time\n",
    "for t in utils.date_generator(start_time, end_time, timedelta(seconds=30)):\n",
    "    if t - temp_time > progress_bar_step:\n",
    "        temp_time = t\n",
    "        utils.print_progress_bar(\n",
    "            end_time, t, \"Calculate the co-occurrence matrix of labels.\"\n",
    "        )\n",
    "    count += 1\n",
    "    vec = analysis.label_vector(AL_periods, t)\n",
    "    vec = vec.reshape((len(vec), 1))\n",
    "    sum_mat += vec @ (vec.T)\n",
    "analysis.matrix2image(path, \"label_cooccurrence\", sum_mat / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-christian",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "- 部分的なセンサの時系列パターン [1] D. J. Cook, N. C. Krishnan and P. Rashidi, \"Activity Discovery and Activity Recognition: A New Partnership,\" in IEEE Transactions on Cybernetics, vol. 43, no. 3, pp. 820-828,\n",
    "- 単位時間におけるセンサの反応頻度，推定した移動距離，無反応時間間隔 [2] Detection of Abnormal Living Patterns for Elderly Living Alone Using Support Vector Data Description\n",
    "- センサ位置から推定した滞在箇所，滞在継続時間 [3] Anomaly Detection Algorithm Based on Life Pattern Extraction from Accumulated Pyroelectric Sensor Data\n",
    "上記のものや特徴量選択\n",
    "- ベクトル化 [4] Activity2vec, [5] sensor2vec\n",
    "\n",
    "to do\n",
    "- [2] の特徴量を抽出する関数の作成\n",
    "- センサデータからの場所列の推定（行動認識）\n",
    "- [3] を実験できるか試す\n",
    "- 今までのアイデアをまとめる。サンプリング幅の異なる複数異常をオンラインに検出するアルゴリズムを作りたい。サンプリング幅の次元を追加する？\n",
    "- Toy dataset を作って確かめる必要があるかどうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bae9b-94dc-4494-813a-41e618fb2520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
